[{"authors":["pieter-delobelle"],"categories":null,"content":"","date":1588591843,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1588591843,"objectID":"0530f3e222950a81b568838eb0e064ce","permalink":"https://dtai.cs.kuleuven.be/stories/authors/pieter-delobelle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/stories/authors/pieter-delobelle/","section":"authors","summary":"","tags":null,"title":"Pieter Delobelle","type":"authors"},{"authors":["thomas-winters"],"categories":null,"content":"","date":1588591843,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1588591843,"objectID":"d568688abf4efe2bb5691f67754a2d1f","permalink":"https://dtai.cs.kuleuven.be/stories/authors/thomas-winters/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/stories/authors/thomas-winters/","section":"authors","summary":"","tags":null,"title":"Thomas Winters","type":"authors"},{"authors":["robin-manhaeve"],"categories":null,"content":"","date":1585643794,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1585643794,"objectID":"11a3d20af860ccdc26e5f6bc140a5b7a","permalink":"https://dtai.cs.kuleuven.be/stories/authors/robin-manhaeve/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/stories/authors/robin-manhaeve/","section":"authors","summary":"","tags":null,"title":"Robin Manhaeve","type":"authors"},{"authors":["hendrik"],"categories":null,"content":"","date":1585065501,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1588238309,"objectID":"2d0d4917731f5e00d32ebf468adb5b8f","permalink":"https://dtai.cs.kuleuven.be/stories/authors/hendrik/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/stories/authors/hendrik/","section":"authors","summary":"","tags":null,"title":"Hendrik Blockeel","type":"authors"},{"authors":["wannes"],"categories":null,"content":"","date":1585065501,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1588238309,"objectID":"bc43e3935adb05adf500b01bf53e7448","permalink":"https://dtai.cs.kuleuven.be/stories/authors/wannes/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/stories/authors/wannes/","section":"authors","summary":"","tags":null,"title":"Wannes Meert","type":"authors"},{"authors":["jdavis","jesse-davis"],"categories":null,"content":"","date":1585038994,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1585038994,"objectID":"495f2ae7aa8d046934fbe84de6cd7f22","permalink":"https://dtai.cs.kuleuven.be/stories/authors/jesse-davis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/stories/authors/jesse-davis/","section":"authors","summary":"","tags":null,"title":"Jesse Davis","type":"authors"},{"authors":["vincentv","vincent-vercruyssen"],"categories":null,"content":"","date":1585038994,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1585038994,"objectID":"6b0077f0b33ddfad9e3371b71b87df61","permalink":"https://dtai.cs.kuleuven.be/stories/authors/vincent-vercruyssen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/stories/authors/vincent-vercruyssen/","section":"authors","summary":"","tags":null,"title":"Vincent Vercruyssen","type":"authors"},{"authors":["demo"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"9df00a99fdbee96626c7ddb68765a08a","permalink":"https://dtai.cs.kuleuven.be/stories/authors/demo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/stories/authors/demo/","section":"authors","summary":"","tags":null,"title":"Demo","type":"authors"},{"authors":[],"categories":["Sports Analytics"],"content":"Expected goals (xG) measures the quality of a shot attempt in soccer based on several variables such as the shot\u0026rsquo;s angle and distance from goal, whether it was a headed shot or a free kick, etc. Adding up a player or team\u0026rsquo;s xG can give an indication of how many goals a player or team should have scored on average, given the shots they have taken. It has become the most used and best understood advanced metric in the world of football, but training an accurate xG model is not as simple as it may seem. In series of blog posts, we discuss the technical aspects of data selection, feature modelling and model evaluation.\n HOW DATA AVAILABILITY AFFECTS THE ABILITY TO LEARN GOOD XG MODELS The available training data can affect the quality of an xG model. This blogpost answers 3 questions:\n How much data is needed to train an xG model? Does data go out of date? Is there a league-specific effect?  See the full post at https://dtai.cs.kuleuven.be/sports/blog/how-data-availability-affects-the-ability-to-learn-good-xg-models\n    ILLUSTRATING THE INTERPLAY BETWEEN FEATURES AND MODELS IN XG The shot\u0026rsquo;s location is among the most important and predictive features in an xG model. How one should encode this location is not trivial and requires a good understanding of some key ML concepts.\nSee the full post at https://dtai.cs.kuleuven.be/sports/blog/illustrating-the-interplay-between-features-and-models-in-xg\n   ","date":1589618249,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589618249,"objectID":"0682bcb37a13190518b0c7c7e146bd11","permalink":"https://dtai.cs.kuleuven.be/stories/post/sports/xg/","publishdate":"2020-05-16T10:37:29+02:00","relpermalink":"/stories/post/sports/xg/","section":"post","summary":"Expected goals (xG) measures the quality of a shot attempt in soccer based on several variables. They are among the most used and best understood advanced metric in the world of football, but training an accurate xG model is not as simple as it may seem.","tags":["Sports","Soccer"],"title":"Pitfalls in training and evaluating expected goals (xG) models","type":"post"},{"authors":["Pieter Delobelle","Thomas Winters","Bettina Berendt"],"categories":["Texts and Web"],"content":"  This post is based on the paper: RobBERT: A Dutch RoBERTa-based Language Model. Pieter Delobelle, Thomas Winters, Bettina Berendt. ArXiv Preprint, 2020.\nThis blog post is based on a more extensive blog post, along with an introduction in language models.\n   What is RobBERT? The advent of neural networks in natural language processing (NLP) has significantly improved state-of-the-art results within the field. While recurrent neural networks (RNNs) and long short-term memory networks (LSTMs) initially dominated the field, recent models started incorporating attention mechanisms and then later dropped the recurrent part and just kept the attention mechanisms in so-called transformer models. This latter type of model caused a new revolution in NLP and led to popular language models like GPT-2 and ELMo. BERT improved over previous transformer models and recurrent networks by allowing the system to learn from input text in a bidirectional way, rather than only from left-to-right or the other way around. This model was later re-implemented, critically evaluated and improved in the RoBERTa model.\nThese large-scale transformer models provide the advantage of being able to solve NLP tasks by having a common, expensive pre-training phase, followed by a smaller fine-tuning phase. The pre-training happens in an unsupervised way by providing large corpora of text in the desired language. The second phase only needs a relatively small annotated data set for fine-tuning to outperform previous popular approaches in one of a large number of possible language tasks.\nWhile language models are usually trained on English data, some multilingual models also exist. These are usually trained on a large quantity of text in different languages. For example, Multilingual-BERT is trained on a collection of corpora in 104 different languages and generalizes language components well across languages. However, models trained on data from one specific language usually improve the performance over multilingual models for this particular language. Training a RoBERTa model on a Dutch dataset thus has a lot of potential for increasing performance for many downstream Dutch NLP tasks.\nLanguage modeling with encoders In NLP, encoder-decoder models have been used for some time. These models, often called sequence-to-sequence or seq2seq, are good at various sequence-based tasks: translations, token labeling, named entity recognition (NER), etc. Historically, these seq2seq models were usually LSTMs or other recurrent networks. A major improvement in these networks was an attention mechanism, that allowed to communicate more than one feature vector. (For those coming from computer vision, this looks a bit like the connections in UNet).\nThe by now famous transformer model was based solely on this attention mechanism. It features 2 stacks: (i) an encoder stack that uses multiple layers of self-attention and (ii) a decoder stack with attention layers that connect back to the encoder outputs.\nWe could also interpret this probabilistically, we have a language model\n$$P(\\text{“giraf\u0026rdquo;}|\\text{“ik zie een \u0026lt;mask\u0026gt; in mijn tuin.\u0026quot;})\u0026lt;0.0001$$\nOr a more probable suggestion:\n$$P(\\text{boom\u0026rdquo;}|\\text{“ik zie een \u0026lt;mask\u0026gt; in mijn tuin.\u0026quot;})\u0026lt;0.0001$$\nIn fact, we can even query the model to get the most likely results. For this sentence, RobBERT gives us:\n[('Ik zie een lamp in mijn tuin.', 0.39584335684776306, ' lamp'), ('Ik zie een boom in mijn tuin.', 0.1497979462146759, ' boom'), ('Ik zie een camera in mijn tuin.', 0.089895099401474, ' camera'), ('Ik zie een ster in mijn tuin.', 0.046020057052373886, ' ster'), ('Ik zie een stip in mijn tuin.', 0.009481011889874935, ' stip'), ('Ik zie een man in mijn tuin.', 0.009198895655572414, ' man'), ('Ik zie een slang in mijn tuin.', 0.009129301644861698, ' slang'), ('Ik zie een stem in mijn tuin.', 0.007939961738884449, ' stem'), ('Ik zie een bos in mijn tuin.', 0.007785357069224119, ' bos'), ('Ik zie een storm in mijn tuin.', 0.0077188946306705475, ' storm')]  Fine-tuning and custom use-cases As it happens, language models are quite expensive to train. We used a high-performance computing cluster with ~80 Nvidia P100’s for several days. This is because we train it on a large dataset (39 GB of text) with the word-masking objective, which means we randomly replace some words by a \u0026lt;mask\u0026gt; token (or another word or the same word, but those are less likely). After a few epochs, we have something that resembles the probabilistic language model we described earlier.\nBut this language model can do more than just filling in some \u0026lt;mask\u0026gt; tokens! This is one of the so-called heads, the one we use to pre-train our language model on. After training, we can easily take it off—perhaps calling them heads was a bit insensitive?—and replace it another one. This step is then called finetuning. All the weights of the model stay the same and we add a newly initialized head that we train on the data that we want. And since most weights are from the trained base model, we only need a fraction of the data. So it will go a lot faster as well!\nYou can also fine-tune RobBERT on your own (Dutch) data, which can save you a lot of training time and you\u0026rsquo;ll likely need fewer labeled examples. Some projects are already using RobBERT in the wild:\n Detecting new job titles for VDAB: Jeroen Van Hautte, Vincent Schelstraete, and Mikaël Wornoo, ‘Leveraging the Inherent Hierarchy of Vacancy Titles for Automated Job Ontology Expansion’, ArXiv:2004.02814, 6 April 2020. \u0026lsquo;Die\u0026rsquo; versus \u0026lsquo;dat\u0026rsquo; disambiguation for L2 learners: Small pilot with the \u0026lsquo;Die\u0026rsquo; vs. \u0026lsquo;dat\u0026rsquo; disambiguation model to give feedback to second language learners. Implemented as a REST api to be used alongside a rule-baed system.  Results We evaluated RobBERT on two tasks with Dutch text inputs: (i) sentiment analysis and distinguishing between die and dat. Table 1 shows the results, where we trained most models twice. Once on the full training set and once on a smaller subset, to highlight a benefit of monolingual language resources over multilingual ones.\n ","date":1588591843,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588591843,"objectID":"dad5fa311b77efebcc260b3b8e6afc3c","permalink":"https://dtai.cs.kuleuven.be/stories/post/pieter-delobelle/robbert/","publishdate":"2020-05-04T13:30:43+02:00","relpermalink":"/stories/post/pieter-delobelle/robbert/","section":"post","summary":"We have developed RobBERT, a language model pre-trained on Dutch texts. This post will give a brief overview of DTAI's work in this field.","tags":["Language modeling","Natural language processing","BERT"],"title":"RobBERT: a Dutch Language Model","type":"post"},{"authors":[],"categories":["Sports Analytics"],"content":"VAEP (Valuing Actions by Estimating Probabilities) is a framework for valuing player actions in soccer. It assigns a value to each on-the-ball action in event stream data based on its impact on the game outcome while accounting for the context in which the action happened. In the series of blog posts below, we explain the methodology behind VAEP, how VAEP relates to other similar frameworks and use it to analyse a season of the English Premier League and Belgian Jupiler Pro League.\n EXPLORING HOW VAEP VALUES ACTIONS This blog post introduces an interactive visualization that explains the methodology behind VAEP and a tool to explore how VAEP values player actions in soccer.\nSee the full post at https://dtai.cs.kuleuven.be/sports/blog/exploring-how-vaep-values-actions\nThe interactive can be viewed at https://dtai.cs.kuleuven.be/sports/vaep\n    INTRODUCING ATOMIC-SPADL: A NEW WAY TO REPRESENT EVENT STREAM DATA This blog post discusses Atomic-SPADL \u0026ndash; an alternative data representation for soccer event stream data \u0026ndash; and how it affects learning models like VAEP.\nSee the full post at https://dtai.cs.kuleuven.be/sports/blog/introducing-atomic-spadl:-a-new-way-to-represent-event-stream-data\n    A CRITICAL COMPARISON OF XT AND VAEP This blog post contextualizes VAEP within the existing work and discusses how it differs from ball-progression frameworks such as xT.\nSee the full post at https://dtai.cs.kuleuven.be/sports/blog/valuing-on-the-ball-actions-in-soccer:-a-critical-comparison-of-xt-and-vaep\n    OUR THOUGHTS ON AMERICAN SOCCER ANALYSIS\u0026rsquo; G+ METRIC Continuing on the blog post above, this post discusses how VAEP differs from American Soccer Analysis\u0026rsquo; g+ metric\nSee the full post at https://dtai.cs.kuleuven.be/sports/blog/our-thoughts-on-american-soccer-analysis'-g+-metric\n    A DATA-DRIVEN REVIEW OF THE FIRST PREMIER LEAGUE HALF A short analysis of the first half of the 2019/2020 Premier League season to identify the best team, the best signings, the rising talents and some of the highest-rated passes.\nSee the full post at https://dtai.cs.kuleuven.be/sports/blog/a-data-driven-review-of-the-first-premier-league-half\n    EEN TERUGBLIK OP DE JUPILER PRO LEAGUE An analysis of the 2019/2020 season of the Belgian Jupiler Pro League. We discuss how teams evolved during the season, which players performed best and identify the most exciting games.\nSee the full post (in Dutch) at https://dtai.cs.kuleuven.be/sports/blog/een-terugblik-op-de-jupiler-pro-league\n   ","date":1587976649,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587976649,"objectID":"ca0e4cb2e0262b187bf22ac1b6b902ee","permalink":"https://dtai.cs.kuleuven.be/stories/post/sports/exploring-how-vaep-values-actions/","publishdate":"2020-04-27T10:37:29+02:00","relpermalink":"/stories/post/sports/exploring-how-vaep-values-actions/","section":"post","summary":"The VAEP framework is a novel data-driven framework for assessing the impact of the individual actions performed by soccer players during a game, which is a crucial aspect of the player recruitment process. Here we group a number of blogposts on the methodology and applications of VAEP.","tags":["Sports","Soccer"],"title":"Valuing actions in soccer","type":"post"},{"authors":["Robin Manhaeve"],"categories":["Probabilistic Programming and Statistical Relational Learning"],"content":"This is a brief overview of DeepProbLog, a neuro-symbolic framework that integrates the probabilistic logic programming language ProbLog with neural networks. The full paper can be found here.\nWhat is DeepProbLog? DeepProbLog is an extension of ProbLog that integrates neural networks through the concept of the neural predicate. It allows us to combine high-level logical reasoning with the sub-symbolic power of neural networks.\nLet\u0026rsquo;s say we want to solve the following problem. We get two handwritten digits (MNIST) and have to determine its sum, e.g. This task can be easily handled in DeepProbLog:\nnn(mnist_classifier,[X],Y,[0..9]) :: digit(X,Y). addition(X,Y,Z) :- digit(X,N1), digit(Y,N2), Z is N1+N2.  Where the first line is a neural predicate that includes a neural network as a predicate inside of the logical program. This neural network can be trained. The addition predicate defines the sum of the individual digits. We can now train this DeepProbLog model and all of the neural networks inside of it using examples of additions. It\u0026rsquo;s important to note that the neural networks will, thanks to the logic, learn to recognize individual digits.\nThe main strengths of DeepProbLog are:\n It combines probabilistic reasoning, logical reasoning and the power of neural networks. It can train neural networks and learn probabilistic paramters from examples. We retain both logic and neural networks as edge cases.  Results MNIST addition We compared the MNIST addition example describe above with a convolutional neural network baseline. The result shows that the inclusion of the logic allows the model to train quicker and achieve a higher accuracy. It\u0026rsquo;s also important to note that the neural network trained inside the DeepProbLog model can recognize single digits, whereas the convolutional baselines can only classify sums. The separation between the logic and neural aspects results in a more flexible model. Sketching We reimplemented the experiments from the Differentiable Forth paper. These use a sketching approach to learn the missing behaviour from partial programs using small neural modules.\nFrom the result we can see that we perform similar to the original (neural) model. For learning to sort lists, Differentiable Forth starts to struggle starting from length 4. This is due to the long program trace. DeepProbLog does not have this problem thanks to the fact that it can perform almost all of the program in the logic. ","date":1585643794,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585643794,"objectID":"a1f4f382015edb088f89eeef2d1a3fc7","permalink":"https://dtai.cs.kuleuven.be/stories/post/robin-manhaeve/deepproblog/","publishdate":"2020-03-31T10:36:34+02:00","relpermalink":"/stories/post/robin-manhaeve/deepproblog/","section":"post","summary":"This is a brief overview of DeepProbLog, a neuro-symbolic framework that integrates the probabilistic logic programming language ProbLog with neural networks.","tags":["ProbLog","Neuro-symbolic","Probabilistic Programming"],"title":"DeepProbLog","type":"post"},{"authors":["Wannes Meert","Hendrik Blockeel"],"categories":["Anomaly Detection"],"content":"  This post is based on the following publications:\n COBRAS-TS: A new approach to Semi-Supervised Clustering of Time Series. Van Craenendonck, T., Meert, W., Dumancic, S. \u0026amp; Blockeel, H. Discovery Science, 2018.  Software is available in the following toolboxes: COBRAS, DTAIDistance\n   Clustering is one of the most widely used techniques for time series because it allows to identify and summarize patterns that are of interest (e.g., frequent or anomalous patterns). Furthermore, it does not rely on costly human supervision of time-consuming labeling. As a result, time series clustering has been studied for many different applications such as astronomy, biology, meteorology, medicine, finance, robotics, engineering, etc..\nUnsupervised Clustering: DTW Time series clustering is heavily dependent on the choice of the distance used to compare two series. Typically, one is interested in similarity between shapes represented by a series, irrespective of phase or amplitude. And while many distance measures have been proposed, it has been shown that distance measures that can deal with invariances to amplitude and phase perform particularly well. One of the best performing similarity measures is Dynamic Time Warping (DTW).\n  Dynamic Time Distance: Warping between two series   The resulting clustering is robust against small changes between time series. As can be seen in the figure underneath, DTW allows a clustering that nicely groups similar series.\n  Time series clustering with DTW   The toolbox is available at https://github.com/wannesm/dtaidistance/ .\nSemi-supervised Clustering: COBRAS Clustering is ubiquitous in data analysis. There is a large diversity in algorithms, loss functions, similarity measures, etc. This is partly due to the fact that clustering is inherently subjective: in many cases, there is no single correct clustering, and different users may prefer different clusterings, depending on their goals and prior knowledge [17]. Depending on their preference, they should use the right algorithm, similarity measure, loss function, hyperparameter settings, etc. This requires a fair amount of knowledge and expertise on the user\u0026rsquo;s side. Semi-supervised clustering methods deal with this subjectiveness in a differ- ent manner. They allow the user to specify constraints that express their subjective interests. These constraints can then guide the algorithm towards solutions that the user finds interesting. Many such systems obtain these constraints by asking the user to answer queries of the following type: should these two elements be in the same cluster? A so-called must-link constraint is obtained if the answer is yes, a cannot-link otherwise. In many situations, answering this type of questions is much easier for the user than selecting the right algorithm, defining the similarity measure, etc. Active semi-supervised clustering methods aim to limit the number of queries that is required to obtain a good clustering by selecting informative pairs to query. In the context of clustering time series, the subjectiveness of clustering is even more prominent. In some contexts, the time scale matters, in other contexts it does not. Similarly, the scale of the amplitude may (not) matter. One may want to cluster time series based on certain types of qualitative behavior (monotonic, periodic, \u0026hellip;), local patterns that occur in them, etc. Despite this variability, and although there is a plethora of work on time series clustering, semi-supervised clustering of time series has only very recently started receiving attention.\n  Clustering is ubiquitous in data analysis, including analysis of time series. But it is inherently subjective: different users may prefer different clusterings for a particular dataset. Semi-supervised clustering addresses this by allowing the user to provide examples of instances that should (not) be in the same cluster.\nThe COBRAS-TS algorithm is a semi-supervised clustering method that can use indirect feedback from users. It clusters with the user in the loop. The user can provide indirect feedback where it is only required to tell the system for a few time series whether two time series represent the same behavior or not. As a result the COBRAS-TS system can identify clusters that are characterized by small local patterns.\n     An interface is provided to the user that actively asks for feedback about time series where the method is the most unsure about how to cluster them.\n    The toolbox is available at https://dtai.cs.kuleuven.be/software/cobras/ .\n","date":1585065501,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585065501,"objectID":"adc7dbf2ca920ab47fb19287b943a6e8","permalink":"https://dtai.cs.kuleuven.be/stories/post/wannes/time-series-clustering/","publishdate":"2020-03-24T16:58:21+01:00","relpermalink":"/stories/post/wannes/time-series-clustering/","section":"post","summary":"This post is based on the following publications:\n COBRAS-TS: A new approach to Semi-Supervised Clustering of Time Series. Van Craenendonck, T., Meert, W., Dumancic, S. \u0026amp; Blockeel, H.","tags":["time series","clustering","Application","Open-Source Software"],"title":"Time Series Clustering","type":"post"},{"authors":["Wannes Meert","Vincent Vercruyssen","Jesse Davis"],"categories":["Anomaly Detection","Engineering and Sensors"],"content":"  This post is based on the following publications:\n Semi-supervised Anomaly Detection with an Application to Water Analytics. Vincent Vercruyssen, Wannes Meert, Gust Verbruggen, Koen Maes, Ruben Bäumer, Jesse Davis. IEEE International Conference on Data Mining. Singapore, 2018. Transfer Learning for Anomaly Detection through Localized and Unsupervised Instance Selection. Vincent Vercruyssen, Wannes Meert, Jesse Davis. Thirty-Fourth AAAI Conference on Artificial Intelligence. New York, 2020. \u0026ldquo;Now you see it, now you don\u0026rsquo;t!\u0026rdquo; Detecting Suspicious Pattern Absences in Continuous Time Series. Vincent Vercruyssen, Wannes Meert, Jesse Davis. SIAM International Conference on Data Mining. Cincinnati, 2020.  The results are shared in the following toolboxes: Anomatools toolbox, LocIT, FZapPa\n   Anomaly detection deals with situations where mostly normal behaviour is observed and one is interested in detecting deviations from this normal behaviour. Examples of abnormal behaviour, the anomalies, are often not available because they are expensive to gather (e.g., breaking a machine) or they are not yet known (e.g., adversarial behaviour). Anomaly detection models typically learn what normal behaviour is based on the distribution of the data and a small number of labels proved by an expert (i.e., information about whether the observed behaviour is normal or abnormal).\nDue to recent developments in Industry 4.0 and Internet-of-Things technologies, companies in all types of industries can continuously monitor all aspects of a production process and visualize them in a dashboard. Equipment is monitored using a variety of sensors, natural resource usage is tracked, and interventions are recorded. In this context, a common task is to identify anomalous behavior from the time series data generated by sensors. As manually analyzing such data is laborious and expensive, automated approaches have the potential to be much more efficient as well as cost effective. While anomaly detection could be posed as a supervised learning problem, typically this is not possible as few or no labeled examples of anomalous behavior are available and it is oftentimes infeasible or undesirable to collect them (e.g., you do not want to break an expensive machine to collect labels). Therefore, unsupervised approaches are commonly employed which typically identify anomalies as deviations from normal (i.e., common or frequent) behavior. However, in many real-world settings several types of normal behavior exist that occur less frequently than some anomalous behaviors.\nTherefore, one of the DTAI research lines is focused on augmenting anomaly detection with flexible user feedback. We refer to this type of feedback as \u0026lsquo;flexible\u0026rsquo; because we want to allow the user to:\n Label only part of the instances; Give other feedback than only labels (e.g. these instances are not the same); Label instances from one dataset and reuse them in another dataset (e.g. label data collected for one machine/store and use it for another one).  Including flexible user-feedback Anomaly detection can naturally be posed as an unsupervised learning task. Typically, unsupervised approaches exploit the underlying assumption that anomalies occur infrequently, which means they fall in low-density regions of the instance space, or that anomalies are far away from normal instances to identity them. However, real-world data regularly violate this assumption, degrading the unsupervised approaches’ performance (e.g., system maintenance can occur infrequently and irregularly, but is not anomalous). Labeled data offers the possibility to correct the mistakes made by unsupervised detectors. Unfortunately, a fully supervised approach to anomaly detection is infeasible due to the fact that collecting examples of real-world anomalies is often expensive (e.g., a machine breaking), meaning that is not a viable strategy to permit anomalous behavior for the sake of data generation. This has spurred interest in semi-supervised approaches to anomaly detection, usually in conjunction with active learning to efﬁciently collect the labels.\nSemi-supervised learning Sometimes there is some information available. For example, some labels might be known or the user can answer a few question. An approach used in our research is a constrained-clustering-based approach for anomaly detection that works in both an unsupervised and semi-supervised setting. Starting from an unlabeled data set, the approach is able to gradually incorporate expert-provided feedback to improve its performance.\nThe methods presented here are implemented as part of the Anomatools toolbox.\nReferences:   Semi-supervised Anomaly Detection with an Application to Water Analytics. Vincent Vercruyssen, Wannes Meert, Gust Verbruggen, Koen Maes, Ruben Bäumer, Jesse Davis. IEEE International Conference on Data Mining. Singapore, 17 November 2018.  Transfer Learning Real-world anomaly detection tasks often involve monitoring numerous assets, each of which is only slightly different. Such a situation may arise when monitoring machines in a factory, resource usage in a chain of retail stores, or windturbine farms. These use cases entail monitoring a large number of assets as a big retail chain could have 100s if not 1000s of stores. Therefore, even when using a strategy like active learning, it may be impossible to collect labels for each individual asset. Given that these use cases involve multiple similar anomaly detection tasks, it may be possible to employ transfer learning to transfer labeled instances from one task to another. This could then alleviate the need to collect labels for each task separately.\nMotivated by these types of applications, we developed LocIT, a transfer learning algorithm tailored towards anomaly detection. It works in two steps. First, given a partially labeled source dataset and an unlabeled target dataset, LocIT selects a subset of the labeled source instances to transfer to the target dataset. It picks those instances that have similar localized data distributions in both the source and target dataset. Second, it assigns an anomaly score using a semi-supervised nearest-neighbor approach that considers both the transferred, labeled source instances and the unlabeled target instances.\nThe LocIT method is available at https://github.com/Vincent-Vercruyssen/LocIT.\nReferences:   Transfer Learning for Anomaly Detection through Localized and Unsupervised Instance Selection. Vincent Vercruyssen, Wannes Meert, Jesse Davis. Thirty-Fourth AAAI Conference on Artificial Intelligence. New York, 7 February 2020.  Types of Patterns Missing patterns The standard time series anomaly detection task involves identifying portions of the data characterized by the presence of unexpected or abnormal behavior. The figure underneath (b) illustrates the canonical anomaly detection problem where the grey-shaded region highlights a pattern (i.e., a collection of points) that substantially differs from the other patterns present in the data such as the highlighted green bell-shaped pattern in Figure 1a. In contrast, this paper addresses detecting a radically diﬀerent type of anomalous behavior: the absence of a pattern. The red-shaded region in the figure (c) shows a portion of the time series where we could reasonable expect to see an occurence of the bell-shaped pattern. Detecting such an anomaly is challenging as the observed measurements in the red-shaded region also correspond to typical, normal behavior in the time series.\nIn many real-world use cases, absent patterns often correspond to signiﬁcant anomalies. The next figure shows an illustrative real-world resource monitoring use case. It involves monitoring a retail store\u0026rsquo;s water usage over several weeks. The store\u0026rsquo;s water distribution system contains a device that automatically performs self-cleaning. Although the water usage during a self-cleaning action is always slightly different, the highlighted segments show that a recognizable pattern emerges in the data. A failure of the cleaning action neither causes any immediate problems nor any noticeable eﬀects in the water usage data. However, after a while a sudden breakdown will occur that halts the water distribution and a large amount of water will be disposed through an unmonitored valve. Other real-world use cases, for instance, are found in network monitoring where CPU load is used to monitor data backup operations and recurring failover system tests.\nDetecting this type of anomaly requires determining when a sporadically occurring pattern is absent. Standard approaches struggle in this setting as it violates the common assumption of anomaly detection that frequent behavior is normal and infrequent behavior is abnormal. By definition, the most common behavior of the sporadically occurring pattern is absence. Furthermore, an absent pattern is not necessarily replaced by anomalous behavior.\nDespite the prevalence of this type of anomaly in practice, little attention has been paid toward developing algorithms that can detect the suspicious absence of a pattern. We developed FZapPa, a data-driven algorithm that can automatically detect anomalies characterized by an absent pattern. First, FZapPa views pattern detection through the lens of learning from positive and unlabeled (PU) data. It uses a small number of example occurrences of the pattern to train a PU classiﬁer, and the learned model is used to identify the remaining pattern occurrences. Second, FZapPa analyzes the time series to learn the relevant indirect context in which the pattern is expected to appear and models its occurrence using a probabilistic model. Based on this model, it then predicts an anomaly score to detect absent occurrences.\nFZapPa is available at: https://github.com/Vincent-Vercruyssen/absent_pattern_detection\nReferences  \u0026ldquo;Now you see it, now you don\u0026rsquo;t!\u0026rdquo; Detecting Suspicious Pattern Absences in Continuous Time Series. Vincent Vercruyssen, Wannes Meert, Jesse Davis. SIAM International Conference on Data Mining. Cincinnati, 7 May 2020.  ","date":1585038994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585038994,"objectID":"c2f9f1fa02c552d00baed09e0021ca84","permalink":"https://dtai.cs.kuleuven.be/stories/post/anomaly-detection/anomaly-detection-with-user-feedback/","publishdate":"2020-03-24T10:36:34+02:00","relpermalink":"/stories/post/anomaly-detection/anomaly-detection-with-user-feedback/","section":"post","summary":"An overview of DTAI research on how to handle user feedback for contextual anomaly detection.","tags":["Anomaly Detection","Semi-supervised learning","Outlier detection","Active learning","Flexible supervision","Application","Open-Source Software"],"title":"Anomaly Detection with User Feedback","type":"post"},{"authors":["Thomas Winters"],"categories":["Texts and Web"],"content":"Computational humor Can computers be funny? Certainly your virtual assistant (e.g. Siri or Alexa) is able to tell a joke if you ask for one, but these are of course pre-written, human-made jokes. One might wonder if computers are already advanced enough to understand how to construct good jokes. And if they can write jokes, can they do this in any language and about any topic? And could they tailor their sense of humor to users?\nMany researchers have already looked into humor generation algorithms. Recent popular neural networks approaches seem to indicate that writing good jokes is still far off. Most of the funny things computers create using neural networks, seem to mostly occur on accident.\nFurther in the past, however, more symbolic approaches have been used to generate subjectively better jokes. Researchers created programs to, for example, generate punning riddles, create analogy jokes and detect double entendres. These programs usually define some rules that constitute a funny joke, and then fill in the slots randomly. For example, the STANDUP punning riddle generator might generate a joke like:\n What is the difference between a pretty glove and a silent cat?\nOne is a cute mitten, the other is a mute kitten.\n To create such a joke, the generator uses templates. Template can be seen as sentences with holes, which are later filled in by following certain rules. For example, for the above joke, the template would be the same as the joke, but without the words pretty, glove, silent, cat, cute, mitten, mute and kitten:\n What is the difference between a A B and a C D?\nOne is a E F, the other is a G H.\n These holes are then related to each other using rules, which then fill the holes with appropriate words. For the above template, there are constraints enforcing that E and G end with the same sound, and so should F and H. Similarly, E and H start with the same sound, and so do F and G. To create the question of the riddle, four pairs of synonyms are required, namely A should be a synonym of E, B of F and so on. In a way, the generator is playing Mad Libs with itself, but enforcing slightly more logic in the relations between the words.\nTeaching joke patterns So while it might be hard to make a computer come up with a broad range of clever jokes completely from scratch, we can teach them how to generate specific types of jokes. However, while many English language resources exist, not all languages possess such plentiful language resources for enforcing and checking linguistic constraints. Another problem is that most joke generators use static data sources (e.g. an internal dictionary), and are thus unable to create jokes about topics that are not included in this data, unless they are manually updated. Old joke generators might thus not be able to make jokes about new music artists or politicians.\nWe created bot for generating Dutch \u0026ldquo;Kermit de Kikker\u0026rdquo; punning riddles, using limited Dutch language resources, namely Wikipedia, a thesaurus (or Wiktionary), rhyming dictionary and hyphenation. All these resources tend to be available online for most of the popular languages. The classic \u0026ldquo;Kermit de Kikker\u0026rdquo; (Dutch for Kermit the Frog) joke is based on finding rhymes of Kikker based on what the riddle suggest. For example:\n Het is groen en het plakt?\nKermit de Sticker\n In English, this joke says: \u0026ldquo;It\u0026rsquo;s green and adhesive? Kermit the Sticker\u0026rdquo;. This is a joke because \u0026ldquo;sticker\u0026rdquo; is a rhyme of \u0026ldquo;kikker\u0026rdquo;, Dutch for \u0026ldquo;frog\u0026rdquo;. Usually, this joke is followed by a large succession of similar jokes about Kermit, e.g.\n Het is groen en is pyromaan?\nKermit de Fikker\n As you might have realised, this is something we can teach computers to generate for us. But why stick with only making jokes about Kermit de Kikker when you can insert any name? Given the name Kanye West as input, it would perform the following steps:\n   Find a rhyme on the last word with the same number of syllables, e.g. rest . If the last word of the input has multiple syllables, look for rhymes on any combination of consequent syllables. Prefer more common words using a word frequency list, if this is available in the language of choice.\n  Replace the relevant syllables of the input name with the rhyme word, e.g. Kanye Rest.\n  Use Wikipedia to find a nice description of the entity with the input name. This is not that hard to extract from the Wikipedia page, since the introduction usually start with [entity_name] is/was/are [explanation]. By taking the part after the \u0026ldquo;to be\u0026rdquo; verb, and until any punctuation or start of clause, the program can distill a brief description. For example, it would describe Kanye West as an American rapper.\n  It now only has to describe the rhyme word to complete the pun riddle. To achieve this, either a thesaurus (for short descriptions), or segments from Wiktionary (for longer, more interesting descriptions) could be used. The algorithm should however make sure that the description do not contain the word to guess itself, since that would spoil the fun. The word frequency table could also be used to choose less common (and thus more specific) descriptive words. For example, it could describe rest as \u0026ldquo;relief from work\u0026rdquo;.\n  Now it can fill all these words into the the template, to create the following joke:\n   It\u0026rsquo;s an American rapper and is relief from work?\nKanye Rest\n These jokes tend to become more interesting once you have multiple of them, as they turn into a fun guessing game. Luckely, given that we completely automated the generation process, the described program can easily generate many more jokes about this person.\nWe build a Twitterbot, called MopjesBot, that generates five unique jokes using this schema on a daily basis. It first checks the news for articles, then filters out articles about too sensitive topics, and finally picks the name that occurs most in these articles. The complete overview of all steps it follows to generate these jokes are summarised in the diagram below:\nThis system is thus able to generate jokes following a specific template and schema, but also nudges the jokes to have a higher probability of having certain characteristics (e.g. common or less common words in certain template \u0026ldquo;holes\u0026rdquo;).\nLearning what constitutes a joke While it\u0026rsquo;s wonderful that we can already make computers generate jokes using templates and schemas, implementing such joke generators requires a large amount of human effort. The computer is also not really gaining insights into humor itself, but rather the human giving the machine explicit insights into a specific type of joke.\nSo, could it learn these insights by itself? This is one task we want to tackle in the future, which can be subdivided in multiple parts:\n can we automatically extract meaningful relations between words of a good joke? can we find out which jokes are better than others by learning probabilities, and use these to \u0026ldquo;nudge\u0026rdquo; the generators into generating better jokes?  The former is something we have explored in the past, and are still actively investigating. The latter task could be achieve using preference learning. Preference learning is a task where given a set of two data points, the algorithm has to predict which one is preferred by a human. This could then be used to find optimal parameters to contruct a joke that a particular user or group of users might like.\nThe future of automatic joke generation is thus exciting, and still full of opportunities. We can already create joke generation algorithms by hand and getting close to learn them automatically. In the future, we might not need to listen to pre-written jokes told by Siri any more, and instead could enjoy personalised, generated humor. Or, as our algorithm might say:\n It\u0026rsquo;s a branch of artificial intelligence which uses computers in humor research and is a claim of questionable accuracy?\nComputational Rumor\n ","date":1585003948,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585003948,"objectID":"88d24bd7d10b9fd516be3e0f44843cb2","permalink":"https://dtai.cs.kuleuven.be/stories/post/thomas-winters/computational_humor_mopjesbot/","publishdate":"2020-03-23T23:52:28+01:00","relpermalink":"/stories/post/thomas-winters/computational_humor_mopjesbot/","section":"post","summary":"Computational humor Can computers be funny? Certainly your virtual assistant (e.g. Siri or Alexa) is able to tell a joke if you ask for one, but these are of course pre-written, human-made jokes.","tags":["Computational Humor","Computational Creativity"],"title":"Can computers create jokes?","type":"post"},{"authors":["lucdr"],"categories":["Probabilistic Programming and Statistical Relational Learning"],"content":"  This post is based on the following publications:\n ProbLog2: Probabilistic logic programming, Dries, Anton; Kimmig, Angelika; Meert, Wannes; Renkens, Joris; Van den Broeck, Guy; Vlasselaer, Jonas; De Raedt, Luc. Lecture ECML PKDD Demo. Notes in Computer Science book series (LNCS, volume 9286), 2015. Inference and learning in probabilistic logic programs using weighted Boolean formulas, Daan Fierens, Guy Van den Broeck, Joris Renkens, Dimitar Shterionov, Bernd Gutmann, Ingo Thon, Gerda Janssens, and Luc De Raedt. Theory and Practice of Logic Programming, 2015. ProbLog: A probabilistic Prolog and its application in link discovery, L. De Raedt, A. Kimmig, and H. Toivonen, Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07), Hyderabad, India, pages 2462-2467, 2007.  Software is available on the ProbLog website.\n   Probabilistic programming studies probabilistic extensions of programming languages that can be used to model rich domains. The lab has contributed a probabilistic extension of a Prolog, called ProbLog. The resulting language is quite close to a probabilistic database as it imposes probabilities on facts and uses (deterministic) rules to specify dependencies and consequences. Within ProbLog, many techniques for inference and learning have been contributed and ProbLog has also been used in applications in robotics, natural language processing, and bioinformatics.\nhttps://dtai.cs.kuleuven.be/problog\n","date":1577894301,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585065501,"objectID":"e015ad1723bc06181831b54933f47a98","permalink":"https://dtai.cs.kuleuven.be/stories/post/problog/problog/","publishdate":"2020-01-01T16:58:21+01:00","relpermalink":"/stories/post/problog/problog/","section":"post","summary":"This post is based on the following publications:\n ProbLog2: Probabilistic logic programming, Dries, Anton; Kimmig, Angelika; Meert, Wannes; Renkens, Joris; Van den Broeck, Guy; Vlasselaer, Jonas; De Raedt, Luc.","tags":["Probabilistic Programming","ProbLog","SRL"],"title":"ProbLog: Probabilistic Logic Programming","type":"post"},{"authors":["Wannes Meert","Hendrik Blockeel"],"categories":["Games"],"content":"This games was explored in the course \u0026ldquo;Machine Learning: Project\u0026rdquo; (Prof. Hendrik Blockeel, Dr. Wannes Meert). Students developed agents that are able to play the Dots-and-Boxes game.\nThe game implementation is available at https://github.com/wannesm/dotsandboxes (and can be played interactively).\n  Rules of the game  Starting with an empty grid of dots, two players take turns adding a single horizontal or vertical line between two unjoined adjacent dots. The player who completes the fourth side of a 1×1 box earns one point and takes another turn. (A point is typically recorded by placing a mark that identifies the player in the box, such as an initial.) The game ends when no more lines can be placed. The winner is the player with the most points. The board may be of any size. When short on time, a 2×2 board (a square of 9 dots) is good for beginners. A 5×5 is good for experts. ( Wikipedia)\n    While the game has very simple rules, it has a large number of possible states (more speciﬁcally, $|S| = 2^{r(c+1)+(r+1)c}$ states for an $r \\times c$ game), and an even larger number ($\\log_2(|S|)!$) of unique games can be played. No general strategy for optimal play is known. Currently, a 4 × 5 game is the largest game that is solved. (Barker, J. K. and Korf, R. E. Solving dots-and-boxes. AAAI 2012)\n Agents The agents available in the Dots and Boxes demo are developed by students from the Master of Computer Science at KU Leuven as part of the 2017-2018 course Machine Learning Project, taught by Prof. Hendrik Blockeel and Wannes Meert.\nThe machine learning technologies used in the agents are:\n Monte-Carlo Tree Search to simulate game-play Q-learning to learn about good actions Artificial Neural Net / Deep learning to learn about good board states Logic rules to reason about domain specific concepts such as ‘chains’ and ‘sacrificing\u0026rsquo;  Credits Hendrik Blockeel, Wannes Meert, Pieter Robberechts, Arne De Brabandere and Sebastijan Dumančić contributed to this course.\n","date":1575105509,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588238309,"objectID":"c8da1d480d9e4cb09588756587cb07c5","permalink":"https://dtai.cs.kuleuven.be/stories/post/wannes/ml-student-projects-dots-and-boxes/","publishdate":"2019-11-30T11:18:29+02:00","relpermalink":"/stories/post/wannes/ml-student-projects-dots-and-boxes/","section":"post","summary":"This games was explored in the course \u0026ldquo;Machine Learning: Project\u0026rdquo; (Prof. Hendrik Blockeel, Dr. Wannes Meert). Students developed agents that are able to play the Dots-and-Boxes game.\nThe game implementation is available at https://github.","tags":["Master student","Master course","Education"],"title":"ML Student Projects: Dots-and-Boxes","type":"post"},{"authors":[],"categories":["Sports Analytics"],"content":"  We give an intuitive explanation of our recent ECMLPKDD 2019 paper on characterizing soccer players’ style of play based on event stream data, provide illustrative use cases and release a small interactive demo.\nSee the full post at https://dtai.cs.kuleuven.be/sports/blog/player-vectors-characterizing-soccer-players-playing-style\nA demo is available at https://dtai.cs.kuleuven.be/sports/player_vectors\n   ","date":1573547849,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573547849,"objectID":"ca5ca8a28aeaab664b9a8b40822215a9","permalink":"https://dtai.cs.kuleuven.be/stories/post/sports/player-vectors-characterizing-soccer-players-playing-style/","publishdate":"2019-11-12T10:37:29+02:00","relpermalink":"/stories/post/sports/player-vectors-characterizing-soccer-players-playing-style/","section":"post","summary":"A player’s style of play is a reoccurring concept when discussing soccer as each player approaches the game in a different way. Unfortunately, playing style is inherently a subjective and nebulous concept, which makes it difficult to capture it in a data-driven manner. This post gives an intuitive explanation of our recent ECMLPKDD 2019 paper on this topic, provides illustrative use cases and introduces a small interactive demo. ","tags":["Sports","Soccer"],"title":"Player vectors: Characterizing soccer players' playing styles","type":"post"}]