<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Texts and Web | DTAI</title><link>https://dtai.cs.kuleuven.be/stories/categories/texts-and-web/</link><atom:link href="https://dtai.cs.kuleuven.be/stories/categories/texts-and-web/index.xml" rel="self" type="application/rss+xml"/><description>Texts and Web</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 04 May 2020 13:30:43 +0200</lastBuildDate><image><url>img/map[gravatar:%!s(bool=false) shape:circle]</url><title>Texts and Web</title><link>https://dtai.cs.kuleuven.be/stories/categories/texts-and-web/</link></image><item><title>RobBERT: a Dutch Language Model</title><link>https://dtai.cs.kuleuven.be/stories/post/pieter-delobelle/robbert/</link><pubDate>Mon, 04 May 2020 13:30:43 +0200</pubDate><guid>https://dtai.cs.kuleuven.be/stories/post/pieter-delobelle/robbert/</guid><description>
&lt;div class="card relpub">
&lt;div class="card-body">
&lt;small>
&lt;p>This post is based on the paper: &lt;a href="https://arxiv.org/pdf/2001.06286.pdf">RobBERT: A Dutch RoBERTa-based Language Model&lt;/a>. Pieter Delobelle, Thomas Winters, Bettina Berendt. ArXiv Preprint, 2020.&lt;/p>
&lt;p>This blog post is based on &lt;a href="https://people.cs.kuleuven.be/~pieter.delobelle/robbert/">a more extensive blog post, along with an introduction in language models&lt;/a>.&lt;/p>
&lt;/small>
&lt;/div>
&lt;/div>
&lt;h1 id="what-is-robbert">What is RobBERT?&lt;/h1>
&lt;p>The advent of neural networks in natural language processing (NLP) has significantly improved state-of-the-art results within the field. While recurrent neural networks (RNNs) and long short-term memory networks (LSTMs) initially dominated the field, recent models started incorporating attention mechanisms and then later dropped the recurrent part and just kept the attention mechanisms in so-called transformer models. This latter type of model caused a new revolution in NLP and led to popular language models like GPT-2 and ELMo. BERT improved over previous transformer models and recurrent networks by allowing the system to learn from input text in a bidirectional way, rather than only from left-to-right or the other way around. This model was later re-implemented, critically evaluated and improved in the RoBERTa model.&lt;/p>
&lt;p>These large-scale transformer models provide the advantage of being able to solve NLP tasks by having a common, expensive pre-training phase, followed by a smaller fine-tuning phase. The pre-training happens in an unsupervised way by providing large corpora of text in the desired language. The second phase only needs a relatively small annotated data set for fine-tuning to outperform previous popular approaches in one of a large number of possible language tasks.&lt;/p>
&lt;p>While language models are usually trained on English data, some multilingual models also exist. These are usually trained on a large quantity of text in different languages. For example, Multilingual-BERT is trained on a collection of corpora in 104 different languages and generalizes language components well across languages. However, models trained on data from one specific language usually improve the performance over multilingual models for this particular language. Training a RoBERTa model on a Dutch dataset thus has a lot of potential for increasing performance for many downstream Dutch NLP tasks.&lt;/p>
&lt;h2 id="language-modeling-with-encoders">Language modeling with encoders&lt;/h2>
&lt;p>In NLP, encoder-decoder models have been used for some time. These models, often called sequence-to-sequence or seq2seq, are good at various sequence-based tasks: translations, token labeling, named entity recognition (NER), etc. Historically, these seq2seq models were usually LSTMs or other recurrent networks. A major improvement in these networks was an attention mechanism, that allowed to communicate more than one feature vector. (For those coming from computer vision, this looks a bit like the connections in UNet).&lt;/p>
&lt;p>The by now famous transformer model was based solely on this attention mechanism. It features 2 stacks: (i) an encoder stack that uses multiple layers of self-attention and (ii) a decoder stack with attention layers that connect back to the encoder outputs.&lt;/p>
&lt;p>&lt;img src="transformers.png" alt="transformers.png">&lt;/p>
&lt;p>We could also interpret this probabilistically, we have a language model&lt;/p>
&lt;p>$$P(\text{“giraf&amp;rdquo;}|\text{“ik zie een &amp;lt;mask&amp;gt; in mijn tuin.&amp;quot;})&amp;lt;0.0001$$&lt;/p>
&lt;p>Or a more probable suggestion:&lt;/p>
&lt;p>$$P(\text{boom&amp;rdquo;}|\text{“ik zie een &amp;lt;mask&amp;gt; in mijn tuin.&amp;quot;})&amp;lt;0.0001$$&lt;/p>
&lt;p>In fact, we can even query the model to get the most likely results. For this sentence, RobBERT gives us:&lt;/p>
&lt;pre>&lt;code>[('Ik zie een lamp in mijn tuin.', 0.39584335684776306, ' lamp'),
('Ik zie een boom in mijn tuin.', 0.1497979462146759, ' boom'),
('Ik zie een camera in mijn tuin.', 0.089895099401474, ' camera'),
('Ik zie een ster in mijn tuin.', 0.046020057052373886, ' ster'),
('Ik zie een stip in mijn tuin.', 0.009481011889874935, ' stip'),
('Ik zie een man in mijn tuin.', 0.009198895655572414, ' man'),
('Ik zie een slang in mijn tuin.', 0.009129301644861698, ' slang'),
('Ik zie een stem in mijn tuin.', 0.007939961738884449, ' stem'),
('Ik zie een bos in mijn tuin.', 0.007785357069224119, ' bos'),
('Ik zie een storm in mijn tuin.', 0.0077188946306705475, ' storm')]
&lt;/code>&lt;/pre>
&lt;h1 id="fine-tuning-and-custom-use-cases">Fine-tuning and custom use-cases&lt;/h1>
&lt;p>As it happens, language models are quite expensive to train. We used a high-performance computing cluster with ~80 Nvidia P100’s for several days. This is because we train it on a large dataset (39 GB of text) with the word-masking objective, which means we randomly replace some words by a &amp;lt;mask&amp;gt; token (or another word or the same word, but those are less likely). After a few epochs, we have something that resembles the probabilistic language model we described earlier.&lt;/p>
&lt;p>But this language model can do more than just filling in some &amp;lt;mask&amp;gt; tokens! This is one of the so-called heads, the one we use to pre-train our language model on. After training, we can easily take it off—perhaps calling them heads was a bit insensitive?—and replace it another one. This step is then called finetuning. All the weights of the model stay the same and we add a newly initialized head that we train on the data that we want. And since most weights are from the trained base model, we only need a fraction of the data. So it will go a lot faster as well!&lt;/p>
&lt;p>&lt;img src="labeling.png" alt="labeling.png">&lt;/p>
&lt;p>You can also fine-tune RobBERT on your own (Dutch) data, which can save you a lot of training time and you&amp;rsquo;ll likely need fewer labeled examples. Some projects are already using RobBERT in the wild:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Detecting new job titles for VDAB&lt;/strong>: Jeroen Van Hautte, Vincent Schelstraete, and Mikaël Wornoo, ‘Leveraging the Inherent Hierarchy of Vacancy Titles for Automated Job Ontology Expansion’,
&lt;a href="http://arxiv.org/abs/2004.02814" target="_blank" rel="noopener">ArXiv:2004.02814&lt;/a>, 6 April 2020.&lt;/li>
&lt;li>&lt;strong>&amp;lsquo;Die&amp;rsquo; versus &amp;lsquo;dat&amp;rsquo; disambiguation for L2 learners&lt;/strong>: Small pilot with the &amp;lsquo;Die&amp;rsquo; vs. &amp;lsquo;dat&amp;rsquo; disambiguation model to give feedback to second language learners.
&lt;a href="https://github.com/iPieter/RobBERT/tree/master/examples/die_vs_data_rest_api" target="_blank" rel="noopener">Implemented as a REST api&lt;/a> to be used alongside a rule-baed system.&lt;/li>
&lt;/ul>
&lt;h1 id="results">Results&lt;/h1>
&lt;p>We evaluated RobBERT on two tasks with Dutch text inputs: (i) sentiment analysis and distinguishing between &lt;em>die&lt;/em> and &lt;em>dat&lt;/em>. Table 1 shows the results, where we trained most models twice. Once on the full training set and once on a smaller subset, to highlight a benefit of monolingual language resources over multilingual ones.&lt;/p>
&lt;p>&lt;img src="results_table.png" alt="results_table.png">&lt;/p>
&lt;h1 id="heading">&lt;/h1></description></item><item><title>Can computers create jokes?</title><link>https://dtai.cs.kuleuven.be/stories/post/thomas-winters/computational_humor_mopjesbot/</link><pubDate>Mon, 23 Mar 2020 23:52:28 +0100</pubDate><guid>https://dtai.cs.kuleuven.be/stories/post/thomas-winters/computational_humor_mopjesbot/</guid><description>&lt;h3 id="computational-humor">Computational humor&lt;/h3>
&lt;p>Can computers be funny?
Certainly your virtual assistant (e.g. &lt;em>Siri&lt;/em> or &lt;em>Alexa&lt;/em>) is able to tell a joke if you ask for one, but these are of course pre-written, human-made jokes.
One might wonder if computers are already advanced enough to understand how to construct good jokes.
And if they can write jokes, can they do this in any language and about any topic?
And could they tailor their sense of humor to users?&lt;/p>
&lt;p>Many researchers have already looked into humor generation algorithms.
Recent popular neural networks approaches seem to indicate that
writing good jokes is
&lt;a href="https://towardsdatascience.com/teaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912" target="_blank" rel="noopener">still far off&lt;/a>.
Most of the funny things computers create using neural networks,
&lt;a href="https://aiweirdness.com/books" target="_blank" rel="noopener">seem to mostly occur on accident&lt;/a>.&lt;/p>
&lt;p>Further in the past, however, more symbolic approaches have been used to generate subjectively better jokes.
Researchers created programs to, for example,
&lt;a href="http://joking.abdn.ac.uk/webversion/welcome.php" target="_blank" rel="noopener">generate punning riddles&lt;/a>,
&lt;a href="http://www.infoivy.com/2013/09/big-data-what-joke-generator-that-is.html" target="_blank" rel="noopener">create analogy jokes&lt;/a>
and
&lt;a href="https://www.popsci.com/technology/article/2011-04/thats-what-she-said-software-recognizes-pervy-double-entendres-automatically/" target="_blank" rel="noopener">detect double entendres&lt;/a>.
These programs usually define some rules that constitute a funny joke, and then fill in the slots randomly.
For example, the
&lt;a href="%28http://joking.abdn.ac.uk/webversion/welcome.php%29">STANDUP punning riddle generator&lt;/a> might generate a joke like:&lt;/p>
&lt;blockquote>
&lt;p>What is the difference between a pretty glove and a silent cat?&lt;/p>
&lt;p>One is a cute mitten, the other is a mute kitten.&lt;/p>
&lt;/blockquote>
&lt;p>To create such a joke, the generator uses templates.
Template can be seen as sentences with holes, which are later filled in by following certain rules.
For example, for the above joke, the template would be the same as the joke, but without the words &lt;em>pretty&lt;/em>, &lt;em>glove&lt;/em>, &lt;em>silent&lt;/em>, &lt;em>cat&lt;/em>, &lt;em>cute&lt;/em>, &lt;em>mitten&lt;/em>, &lt;em>mute&lt;/em> and &lt;em>kitten&lt;/em>:&lt;/p>
&lt;blockquote>
&lt;p>What is the difference between a &lt;strong>A&lt;/strong> &lt;strong>B&lt;/strong> and a &lt;strong>C&lt;/strong> &lt;strong>D&lt;/strong>?&lt;/p>
&lt;p>One is a &lt;strong>E&lt;/strong> &lt;strong>F&lt;/strong>, the other is a &lt;strong>G&lt;/strong> &lt;strong>H&lt;/strong>.&lt;/p>
&lt;/blockquote>
&lt;p>These holes are then related to each other using rules, which then fill the holes with appropriate words.
For the above template, there are constraints enforcing that &lt;strong>E&lt;/strong> and &lt;strong>G&lt;/strong> end with the same sound, and so should &lt;strong>F&lt;/strong> and &lt;strong>H&lt;/strong>.
Similarly, &lt;strong>E&lt;/strong> and &lt;strong>H&lt;/strong> start with the same sound, and so do &lt;strong>F&lt;/strong> and &lt;strong>G&lt;/strong>.
To create the question of the riddle, four pairs of synonyms are required, namely &lt;strong>A&lt;/strong> should be a synonym of &lt;strong>E&lt;/strong>, &lt;strong>B&lt;/strong> of &lt;strong>F&lt;/strong> and so on.
In a way, the generator is playing
&lt;a href="http://www.madlibs.com/" target="_blank" rel="noopener">Mad Libs&lt;/a> with itself, but enforcing slightly more logic in the relations between the words.&lt;/p>
&lt;p>&lt;img src="mopjesbot_drawing.jpg" alt="mopjesbot">&lt;/p>
&lt;h3 id="teaching-joke-patterns">Teaching joke patterns&lt;/h3>
&lt;p>So while it might be hard to make a computer come up with a broad range of clever jokes completely from scratch,
we can teach them how to generate specific types of jokes.
However, while many English language resources exist, not all languages possess such plentiful language resources for enforcing and checking linguistic constraints.
Another problem is that most joke generators use static data sources (e.g. an internal dictionary), and are thus unable to create jokes about topics that are not included in this data, unless they are manually updated.
Old joke generators might thus not be able to make jokes about new music artists or politicians.&lt;/p>
&lt;p>We created
&lt;a href="https://twitter.com/MopjesBot" target="_blank" rel="noopener">bot&lt;/a> for generating Dutch &lt;em>&amp;ldquo;Kermit de Kikker&amp;rdquo;&lt;/em> punning riddles, using limited Dutch language resources,
namely
&lt;a href="https://nl.wikipedia.org/" target="_blank" rel="noopener">Wikipedia&lt;/a>, a
&lt;a href="https://www.mijnwoordenboek.nl/synoniem.php" target="_blank" rel="noopener">thesaurus&lt;/a> (or
&lt;a href="https://nl.wiktionary.org/wiki/Hoofdpagina" target="_blank" rel="noopener">Wiktionary&lt;/a>),
&lt;a href="https://www.mijnwoordenboek.nl/rijmwoordenboek/" target="_blank" rel="noopener">rhyming dictionary&lt;/a> and
&lt;a href="https://www.ushuaia.pl/hyphen/?ln=nl" target="_blank" rel="noopener">hyphenation&lt;/a>.
All these resources tend to be available online for most of the popular languages.
The classic &lt;em>&amp;ldquo;Kermit de Kikker&amp;rdquo;&lt;/em> (&lt;em>Dutch for
&lt;a href="https://en.wikipedia.org/wiki/Kermit_the_Frog" target="_blank" rel="noopener">Kermit the Frog&lt;/a>&lt;/em>) joke is based on finding rhymes of &lt;em>Kikker&lt;/em> based on what the riddle suggest.
For example:&lt;/p>
&lt;blockquote>
&lt;p>Het is groen en het plakt?&lt;/p>
&lt;p>Kermit de Sticker&lt;/p>
&lt;/blockquote>
&lt;p>In English, this joke says: &lt;em>&amp;ldquo;It&amp;rsquo;s green and adhesive? Kermit the Sticker&amp;rdquo;&lt;/em>.
This is a joke because &lt;em>&amp;ldquo;sticker&amp;rdquo;&lt;/em> is a rhyme of &lt;em>&amp;ldquo;kikker&amp;rdquo;&lt;/em>, Dutch for &lt;em>&amp;ldquo;frog&amp;rdquo;&lt;/em>.
Usually, this joke is followed by a large succession of similar jokes about Kermit, e.g.&lt;/p>
&lt;blockquote>
&lt;p>Het is groen en is pyromaan?&lt;/p>
&lt;p>Kermit de Fikker&lt;/p>
&lt;/blockquote>
&lt;p>As you might have realised, this is something we can teach computers to generate for us.
But why stick with only making jokes about &lt;em>Kermit de Kikker&lt;/em> when you can insert any name?
Given the name &lt;em>Kanye West&lt;/em> as input, it would perform the following steps:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>
&lt;a href="https://www.rhymezone.com/r/rhyme.cgi?Word=west&amp;amp;typeofrhyme=perfect&amp;amp;org1=syl&amp;amp;org2=l&amp;amp;org3=y" target="_blank" rel="noopener">Find a rhyme&lt;/a>
on the last word with the same number of syllables, e.g. &lt;em>rest&lt;/em> .
If the last word of the input has multiple syllables, look for rhymes on any combination of consequent syllables.
Prefer more common words using a word frequency list, if this is available in the language of choice.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Replace the relevant syllables of the input name with the rhyme word, e.g. Kanye Rest.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Use
&lt;a href="https://wikipedia.org" target="_blank" rel="noopener">Wikipedia&lt;/a> to find a nice description of the entity with the input name.
This is not that hard to extract from the Wikipedia page, since the introduction usually start with &lt;em>[entity_name] &lt;strong>is/was/are&lt;/strong> [explanation]&lt;/em>.
By taking the part after the &lt;em>&amp;ldquo;to be&amp;rdquo;&lt;/em> verb, and until any punctuation or start of clause, the program can distill a brief description.
For example, it would describe
&lt;a href="https://en.wikipedia.org/wiki/Kanye_West" target="_blank" rel="noopener">Kanye West&lt;/a> as &lt;em>an American rapper&lt;/em>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>It now only has to describe the rhyme word to complete the pun riddle.
To achieve this, either a
&lt;a href="https://www.thesaurus.com/browse/rest" target="_blank" rel="noopener">thesaurus&lt;/a> (for short descriptions),
or segments from
&lt;a href="https://en.wiktionary.org/wiki/rest" target="_blank" rel="noopener">Wiktionary&lt;/a> (for longer, more interesting descriptions) could be used.
The algorithm should however make sure that the description do not contain the word to guess itself, since that would spoil the fun.
The word frequency table could also be used to choose less common (and thus more specific) descriptive words.
For example, it could describe &lt;em>rest&lt;/em> as &lt;em>&amp;ldquo;relief from work&amp;rdquo;&lt;/em>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Now it can fill all these words into the the template, to create the following joke:&lt;/p>
&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>It&amp;rsquo;s an American rapper and is relief from work?&lt;/p>
&lt;p>Kanye Rest&lt;/p>
&lt;/blockquote>
&lt;p>These jokes tend to become more interesting once you have multiple of them, as they turn into a fun guessing game.
Luckely, given that we completely automated the generation process, the described program can easily generate many more jokes about this person.&lt;/p>
&lt;p>We build a Twitterbot, called
&lt;a href="https://twitter.com/MopjesBot" target="_blank" rel="noopener">MopjesBot&lt;/a>, that generates five unique jokes using this schema on a daily basis.
It first checks the news for articles, then filters out articles about too sensitive topics, and finally picks the name that occurs most in these articles.
The complete overview of all steps it follows to generate these jokes are summarised in the diagram below:&lt;/p>
&lt;p>&lt;img src="mopjesbot_flow.png" alt="mopjesbot overview">&lt;/p>
&lt;p>This system is thus able to generate jokes following a specific template and schema,
but also nudges the jokes to have a higher probability of having certain characteristics (e.g. common or less common words in certain template &amp;ldquo;holes&amp;rdquo;).&lt;/p>
&lt;h3 id="learning-what-constitutes-a-joke">Learning what constitutes a joke&lt;/h3>
&lt;p>While it&amp;rsquo;s wonderful that we can already make computers generate jokes using templates and schemas, implementing such joke generators requires a large amount of human effort.
The computer is also not really gaining insights into humor itself, but rather the human giving the machine explicit insights into a specific type of joke.&lt;/p>
&lt;p>So, could it learn these insights by itself?
This is one task we want to tackle in the future, which can be subdivided in multiple parts:&lt;/p>
&lt;ul>
&lt;li>can we automatically extract meaningful relations between words of a good joke?&lt;/li>
&lt;li>can we find out which jokes are better than others by learning probabilities, and use these to &amp;ldquo;nudge&amp;rdquo; the generators into generating better jokes?&lt;/li>
&lt;/ul>
&lt;p>The former is something we have
&lt;a href="https://www.researchgate.net/publication/325432136_Automatic_Joke_Generation_Learning_Humor_from_Examples" target="_blank" rel="noopener">explored in the past&lt;/a>, and are still actively investigating.
The latter task could be achieve using preference learning.
&lt;a href="https://en.wikipedia.org/wiki/Preference_learning" target="_blank" rel="noopener">Preference learning&lt;/a> is a task where given a set of two data points, the algorithm has to predict which one is preferred by a human.
This could then be used to find optimal parameters to contruct a joke that a particular user or group of users might like.&lt;/p>
&lt;p>The future of automatic joke generation is thus exciting, and still full of opportunities.
We can already create joke generation algorithms by hand and getting close to learn them automatically.
In the future, we might not need to listen to pre-written jokes told by Siri any more, and instead could enjoy personalised, generated humor.
Or, as our algorithm might say:&lt;/p>
&lt;blockquote>
&lt;p>It&amp;rsquo;s a branch of artificial intelligence which uses computers in humor research and is a claim of questionable accuracy?&lt;/p>
&lt;p>Computational Rumor&lt;/p>
&lt;/blockquote></description></item></channel></rss>