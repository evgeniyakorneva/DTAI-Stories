<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.8.0"><meta name=description content="We have developed RobBERT, a language model pre-trained on Dutch texts. This post will give a brief overview of DTAI's work in this field."><link rel=alternate hreflang=en-us href=https://dtai.cs.kuleuven.be/stories/post/pieter-delobelle/robbert/><meta name=theme-color content="#1D8DB0"><script src=/stories/js/mathjax-config.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin=anonymous><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin=anonymous async></script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap"><link rel=stylesheet href=/stories/css/academic.css><link rel=manifest href=/stories/index.webmanifest><link rel=icon type=image/png href=/stories/images/icon_hub1a4823b3117ee1925e6e8fad43d8f06_20616_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/stories/images/icon_hub1a4823b3117ee1925e6e8fad43d8f06_20616_192x192_fill_lanczos_center_2.png><link rel=canonical href=https://dtai.cs.kuleuven.be/stories/post/pieter-delobelle/robbert/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@dtai_kuleuven"><meta property="twitter:creator" content="@dtai_kuleuven"><meta property="og:site_name" content="DTAI"><meta property="og:url" content="https://dtai.cs.kuleuven.be/stories/post/pieter-delobelle/robbert/"><meta property="og:title" content="Robbert | DTAI"><meta property="og:description" content="We have developed RobBERT, a language model pre-trained on Dutch texts. This post will give a brief overview of DTAI's work in this field."><meta property="og:image" content="https://dtai.cs.kuleuven.be/stories/post/pieter-delobelle/robbert/featured.png"><meta property="twitter:image" content="https://dtai.cs.kuleuven.be/stories/post/pieter-delobelle/robbert/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2020-05-04T13:30:43+02:00"><meta property="article:modified_time" content="2020-05-04T13:30:43+02:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://dtai.cs.kuleuven.be/stories/post/pieter-delobelle/robbert/"},"headline":"Robbert","image":["https://dtai.cs.kuleuven.be/stories/post/pieter-delobelle/robbert/featured.png"],"datePublished":"2020-05-04T13:30:43+02:00","dateModified":"2020-05-04T13:30:43+02:00","author":{"@type":"Person","name":"Pieter Delobelle"},"publisher":{"@type":"Organization","name":"DTAI","logo":{"@type":"ImageObject","url":"https://dtai.cs.kuleuven.be/stories/images/icon_hub1a4823b3117ee1925e6e8fad43d8f06_20616_192x192_fill_lanczos_center_2.png"}},"description":"We have developed RobBERT, a language model pre-trained on Dutch texts. This post will give a brief overview of DTAI's work in this field."}</script><title>Robbert | DTAI</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/stories/>DTAI</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/stories/>DTAI</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/stories/#hero><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/stories/#featured><span>Highlights</span></a></li><li class=nav-item><a class=nav-link href=/stories/#posts><span>Articles</span></a></li><li class=nav-item><a class=nav-link href=/stories/#categories><span>Research Lines</span></a></li><li class=nav-item><a class=nav-link href=/stories/#tags><span>Tags</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=#><i class="fas fa-search" aria-hidden=true></i></a></li></ul></div></nav><article class=article><div class="article-container pt-3"><h1>Robbert</h1><p class=page-subtitle>A Dutch RoBERTa-based Language Model</p><div class=article-metadata><div><span><a href=/stories/authors/pieter-delobelle/>Pieter Delobelle</a></span>, <span><a href=/stories/authors/thomas-winters/>Thomas Winters</a></span>, <span><a href=/stories/authors/bettina-berendt/>Bettina Berendt</a></span></div><span class=article-date>4 May 2020</span>
<span class=middot-divider></span><span class=article-reading-time>5 min read</span>
<span class=middot-divider></span><span class=article-categories><i class="fas fa-folder mr-1"></i><a href=/stories/categories/texts-and-web/>Texts and Web</a></span></div></div><div class=article-container><div class=article-style><div class="card relpub"><div class=card-body><small><p>This post is based on the paper: <a href=https://arxiv.org/pdf/2001.06286.pdf>RobBERT: A Dutch RoBERTa-based Language Model</a>. Pieter Delobelle, Thomas Winters, Bettina Berendt. ArXiv Preprint, 2020.</p><p>This blog post is based on <a href=https://people.cs.kuleuven.be/~pieter.delobelle/robbert/>a more extensive blog post, along with an introduction in language models</a>.</p></small></div></div><h1 id=what-is-robbert>What is RobBERT?</h1><p>The advent of neural networks in natural language processing (NLP) has significantly improved state-of-the-art results within the field. While recurrent neural networks (RNNs) and long short-term memory networks (LSTMs) initially dominated the field, recent models started incorporating attention mechanisms and then later dropped the recurrent part and just kept the attention mechanisms in so-called transformer models. This latter type of model caused a new revolution in NLP and led to popular language models like GPT-2 and ELMo. BERT improved over previous transformer models and recurrent networks by allowing the system to learn from input text in a bidirectional way, rather than only from left-to-right or the other way around. This model was later re-implemented, critically evaluated and improved in the RoBERTa model.</p><p>These large-scale transformer models provide the advantage of being able to solve NLP tasks by having a common, expensive pre-training phase, followed by a smaller fine-tuning phase. The pre-training happens in an unsupervised way by providing large corpora of text in the desired language. The second phase only needs a relatively small annotated data set for fine-tuning to outperform previous popular approaches in one of a large number of possible language tasks.</p><p>While language models are usually trained on English data, some multilingual models also exist. These are usually trained on a large quantity of text in different languages. For example, Multilingual-BERT is trained on a collection of corpora in 104 different languages and generalizes language components well across languages. However, models trained on data from one specific language usually improve the performance over multilingual models for this particular language. Training a RoBERTa model on a Dutch dataset thus has a lot of potential for increasing performance for many downstream Dutch NLP tasks.</p><h2 id=language-modeling-with-encoders>Language modeling with encoders</h2><p>In NLP, encoder-decoder models have been used for some time. These models, often called sequence-to-sequence or seq2seq, are good at various sequence-based tasks: translations, token labeling, named entity recognition (NER), etc. Historically, these seq2seq models were usually LSTMs or other recurrent networks. A major improvement in these networks was an attention mechanism, that allowed to communicate more than one feature vector. (For those coming from computer vision, this looks a bit like the connections in UNet).</p><p>The by now famous transformer model was based solely on this attention mechanism. It features 2 stacks: (i) an encoder stack that uses multiple layers of self-attention and (ii) a decoder stack with attention layers that connect back to the encoder outputs.</p><p><img src=transformers.png alt=transformers.png></p><p>We could also interpret this probabilistically, we have a language model</p><p>$$P(\text{“giraf&rdquo;}|\text{“ik zie een &lt;mask> in mijn tuin."})&lt;0.0001$$</p><p>Or a more probable suggestion:</p><p>$$P(\text{boom&rdquo;}|\text{“ik zie een &lt;mask> in mijn tuin."})&lt;0.0001$$</p><p>In fact, we can even query the model to get the most likely results. For this sentence, RobBERT gives us:</p><pre><code>[('Ik zie een lamp in mijn tuin.', 0.39584335684776306, ' lamp'),
 ('Ik zie een boom in mijn tuin.', 0.1497979462146759, ' boom'),
 ('Ik zie een camera in mijn tuin.', 0.089895099401474, ' camera'),
 ('Ik zie een ster in mijn tuin.', 0.046020057052373886, ' ster'),
 ('Ik zie een stip in mijn tuin.', 0.009481011889874935, ' stip'),
 ('Ik zie een man in mijn tuin.', 0.009198895655572414, ' man'),
 ('Ik zie een slang in mijn tuin.', 0.009129301644861698, ' slang'),
 ('Ik zie een stem in mijn tuin.', 0.007939961738884449, ' stem'),
 ('Ik zie een bos in mijn tuin.', 0.007785357069224119, ' bos'),
 ('Ik zie een storm in mijn tuin.', 0.0077188946306705475, ' storm')]
</code></pre><h1 id=fine-tuning-and-custom-use-cases>Fine-tuning and custom use-cases</h1><p>As it happens, language models are quite expensive to train. We used a high-performance computing cluster with ~80 Nvidia P100’s for several days. This is because we train it on a large dataset (39 GB of text) with the word-masking objective, which means we randomly replace some words by a &lt;mask> token (or another word or the same word, but those are less likely). After a few epochs, we have something that resembles the probabilistic language model we described earlier.</p><p>But this language model can do more than just filling in some &lt;mask> tokens! This is one of the so-called heads, the one we use to pre-train our language model on. After training, we can easily take it off—perhaps calling them heads was a bit insensitive?—and replace it another one. This step is then called finetuning. All the weights of the model stay the same and we add a newly initialized head that we train on the data that we want. And since most weights are from the trained base model, we only need a fraction of the data. So it will go a lot faster as well!</p><p><img src=labeling.png alt=labeling.png></p><p>You can also fine-tune RobBERT on your own (Dutch) data, which can save you a lot of training time and you&rsquo;ll likely need fewer labeled examples. Some projects are already using RobBERT in the wild:</p><ul><li><strong>Detecting new job titles for VDAB</strong>: Jeroen Van Hautte, Vincent Schelstraete, and Mikaël Wornoo, ‘Leveraging the Inherent Hierarchy of Vacancy Titles for Automated Job Ontology Expansion’,
<a href=http://arxiv.org/abs/2004.02814 target=_blank rel=noopener>ArXiv:2004.02814</a>, 6 April 2020.</li><li><strong>&lsquo;Die&rsquo; versus &lsquo;dat&rsquo; disambiguation for L2 learners</strong>: Small pilot with the &lsquo;Die&rsquo; vs. &lsquo;dat&rsquo; disambiguation model to give feedback to second language learners.
<a href=https://github.com/iPieter/RobBERT/tree/master/examples/die_vs_data_rest_api target=_blank rel=noopener>Implemented as a REST api</a> to be used alongside a rule-baed system.</li></ul><h1 id=results>Results</h1><p>We evaluated RobBERT on two tasks with Dutch text inputs: (i) sentiment analysis and distinguishing between <em>die</em> and <em>dat</em>. Table 1 shows the results, where we trained most models twice. Once on the full training set and once on a smaller subset, to highlight a benefit of monolingual language resources over multilingual ones.</p><p><img src=results_table.png alt=results_table.png></p><h1 id=heading></h1></div><div class=article-tags><a class="badge badge-light" href=/stories/tags/language-modeling/>Language modeling</a>
<a class="badge badge-light" href=/stories/tags/natural-language-processing/>Natural language processing</a>
<a class="badge badge-light" href=/stories/tags/bert/>BERT</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://dtai.cs.kuleuven.be/stories/post/pieter-delobelle/robbert/&text=Robbert" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://dtai.cs.kuleuven.be/stories/post/pieter-delobelle/robbert/&t=Robbert" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Robbert&body=https://dtai.cs.kuleuven.be/stories/post/pieter-delobelle/robbert/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://dtai.cs.kuleuven.be/stories/post/pieter-delobelle/robbert/&title=Robbert" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://web.whatsapp.com/send?text=Robbert%20https://dtai.cs.kuleuven.be/stories/post/pieter-delobelle/robbert/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://dtai.cs.kuleuven.be/stories/post/pieter-delobelle/robbert/&title=Robbert" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=/stories/authors/pieter-delobelle/_avatar_hu30b551a1d0e5c949da75783d59dbbcbc_8191_270x270_fill_q90_lanczos_center.jpg alt="Pieter Delobelle"><div class=media-body><h5 class=card-title><a href=/stories/authors/pieter-delobelle/>Pieter Delobelle</a></h5><h6 class=card-subtitle>PhD student</h6><p class=card-text>My research interests include machine learning, fairness and NLP.</p><ul class=network-icon aria-hidden=true><li><a href=https://twitter.com/pieterdelobelle target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href="https://scholar.google.be/citations?user=MVjJgxAAAAAJ" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://github.com/iPieter target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/pieter-delobelle target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=https://people.cs.kuleuven.be/~pieter.delobelle/ target=_blank rel=noopener><i class="fas fa-link"></i></a></li></ul></div></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=/stories/authors/thomas-winters/avatar_hufcaba37e630ab4e82fb25dbd58bb83a8_129847_270x270_fill_q90_lanczos_center.jpg alt="Thomas Winters"><div class=media-body><h5 class=card-title><a href=/stories/authors/thomas-winters/>Thomas Winters</a></h5><h6 class=card-subtitle>PhD student</h6><p class=card-text>Researching computational creativity systems using symbolic AI methodologies.</p><ul class=network-icon aria-hidden=true><li><a href=https:/twitter.com/thomas_wint target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href="https://scholar.google.com/citations?user=lHfPiwoAAAAJ" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://github.com/twinters target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/thomas-winters/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=http://thomaswinters.be target=_blank rel=noopener><i class="fas fa-link"></i></a></li></ul></div></div><div class="media author-card content-widget-hr"><div class=media-body><h5 class=card-title><a href=/stories/authors/bettina-berendt/></a></h5><ul class=network-icon aria-hidden=true></ul></div></div></div></article><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin=anonymous></script><script>const code_highlighting=true;</script><script>const isSiteThemeDark=false;</script><script>const search_config={"indexURI":"/stories/index.json","minLength":1,"threshold":0.3};const i18n={"no_results":"No results found","placeholder":"Search...","results":"results found"};const content_type={'post':"Posts",'project':"Projects",'publication':"Publications",'talk':"Talks"};</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/stories/js/academic.min.c816d323c3a55093dae0829b44ea1ca8.js></script><div class=container><footer class=site-footer><div class=container><div class="row align-items-center justify-content-center"><div class="d-inline mx-3"><a href=https://dtai.cs.kuleuven.be/ target=_blank><img src=/stories/img/dtai_logo.svg alt=DTAI></a></div><div class="d-inline mx-3"><a href=https://kuleuven.be/ target=_blank><img src=/stories/img/kuleuven_logo.svg alt="KU Leuven"></a></div></div></div></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body></html>