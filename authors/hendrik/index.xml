<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>DTAI</title><link>https://dtai.cs.kuleuven.be/stories/authors/hendrik/</link><atom:link href="https://dtai.cs.kuleuven.be/stories/authors/hendrik/index.xml" rel="self" type="application/rss+xml"/><description>DTAI</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 24 Mar 2020 16:58:21 +0100</lastBuildDate><image><url>img/map[gravatar:%!s(bool=false) shape:circle]</url><title>DTAI</title><link>https://dtai.cs.kuleuven.be/stories/authors/hendrik/</link></image><item><title>Time Series Clustering</title><link>https://dtai.cs.kuleuven.be/stories/post/wannes/time-series-clustering/</link><pubDate>Tue, 24 Mar 2020 16:58:21 +0100</pubDate><guid>https://dtai.cs.kuleuven.be/stories/post/wannes/time-series-clustering/</guid><description>
&lt;div class="card relpub">
&lt;div class="card-body">
&lt;small>
&lt;p>This post is based on the following publications:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://dtai.cs.kuleuven.be/software/cobras/cobras_ts_cameraready.pdf">COBRAS-TS: A new approach to Semi-Supervised Clustering of Time Series&lt;/a>. Van Craenendonck, T., Meert, W., Dumancic, S. &amp;amp; Blockeel, H. Discovery Science, 2018.&lt;/li>
&lt;/ul>
&lt;p>Software is available in the following toolboxes: &lt;a href="https://dtai.cs.kuleuven.be/software/cobras/">COBRAS&lt;/a>, &lt;a href="https://github.com/wannesm/dtaidistance/">DTAIDistance&lt;/a>&lt;/p>
&lt;/small>
&lt;/div>
&lt;/div>
&lt;p>Clustering is one of the most widely used techniques for time series because it allows to identify and summarize patterns that are of interest (e.g., frequent or anomalous patterns). Furthermore, it does not rely on costly human supervision of time-consuming labeling.
As a result, time series clustering has been studied for many different applications such as astronomy, biology, meteorology, medicine, finance, robotics, engineering, etc..&lt;/p>
&lt;h2 id="unsupervised-clustering-dtw">Unsupervised Clustering: DTW&lt;/h2>
&lt;p>Time series clustering is heavily dependent on the choice of the distance used to compare two series. Typically, one is interested in similarity between shapes represented by a series, irrespective of phase or amplitude. And while many distance measures have been proposed, it has been shown that distance measures that can deal with invariances to amplitude and phase perform particularly well.
One of the best performing similarity measures is Dynamic Time Warping (DTW).&lt;/p>
&lt;figure id="figure-dynamic-time-distance-warping-between-two-series">
&lt;a data-fancybox="" href="https://dtai.cs.kuleuven.be/stories/stories/post/wannes/time-series-clustering/dtw_warp_hu56a5075f5e012b20bb536436aa071195_51331_2000x2000_fit_lanczos_2.png" data-caption="Dynamic Time Distance: Warping between two series">
&lt;img data-src="https://dtai.cs.kuleuven.be/stories/stories/post/wannes/time-series-clustering/dtw_warp_hu56a5075f5e012b20bb536436aa071195_51331_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="400" height="290">
&lt;/a>
&lt;figcaption>
Dynamic Time Distance: Warping between two series
&lt;/figcaption>
&lt;/figure>
&lt;p>The resulting clustering is robust against small changes between time series. As can be seen in the figure underneath, DTW allows a clustering that nicely groups similar series.&lt;/p>
&lt;figure id="figure-time-series-clustering-with-dtw">
&lt;a data-fancybox="" href="https://dtai.cs.kuleuven.be/stories/stories/post/wannes/time-series-clustering/dtw_clustering_hu1e289b70cecb573da4502978abb095f3_170841_2000x2000_fit_lanczos_2.png" data-caption="Time series clustering with DTW">
&lt;img data-src="https://dtai.cs.kuleuven.be/stories/stories/post/wannes/time-series-clustering/dtw_clustering_hu1e289b70cecb573da4502978abb095f3_170841_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="620" height="600">
&lt;/a>
&lt;figcaption>
Time series clustering with DTW
&lt;/figcaption>
&lt;/figure>
&lt;p>The toolbox is available at &lt;a href="https://github.com/wannesm/dtaidistance/">https://github.com/wannesm/dtaidistance/&lt;/a> .&lt;/p>
&lt;h2 id="semi-supervised-clustering-cobras">Semi-supervised Clustering: COBRAS&lt;/h2>
&lt;p>Clustering is ubiquitous in data analysis. There is a large diversity in algorithms, loss functions, similarity measures, etc. This is partly due to the fact that clustering is inherently subjective: in many cases, there is no single correct clustering, and different users may prefer different clusterings, depending on their goals and prior knowledge [17]. Depending on their preference, they should use the right algorithm, similarity measure, loss function, hyperparameter settings, etc. This requires a fair amount of knowledge and expertise on the user&amp;rsquo;s side.
Semi-supervised clustering methods deal with this subjectiveness in a differ- ent manner. They allow the user to specify constraints that express their subjective interests. These constraints can then guide the algorithm towards solutions that the user finds interesting. Many such systems obtain these constraints by asking the user to answer queries of the following type: should these two elements be in the same cluster? A so-called must-link constraint is obtained if the answer is yes, a cannot-link otherwise. In many situations, answering this type of questions is much easier for the user than selecting the right algorithm, defining the similarity measure, etc. Active semi-supervised clustering methods
aim to limit the number of queries that is required to obtain a good clustering by selecting informative pairs to query.
In the context of clustering time series, the subjectiveness of clustering is even more prominent. In some contexts, the time scale matters, in other contexts it does not. Similarly, the scale of the amplitude may (not) matter. One may want to cluster time series based on certain types of qualitative behavior (monotonic, periodic, &amp;hellip;), local patterns that occur in them, etc. Despite this variability, and although there is a plethora of work on time series clustering, semi-supervised clustering of time series has only very recently started receiving attention.&lt;/p>
&lt;figure >
&lt;a data-fancybox="" href="https://dtai.cs.kuleuven.be/stories/stories/post/wannes/time-series-clustering/cobras_logo_hu27e11e8a0168bda589412849cb53f1c4_22373_2000x2000_fit_lanczos_2.png" >
&lt;img data-src="https://dtai.cs.kuleuven.be/stories/stories/post/wannes/time-series-clustering/cobras_logo_hu27e11e8a0168bda589412849cb53f1c4_22373_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="200px" height="350">
&lt;/a>
&lt;/figure>
&lt;p>Clustering is ubiquitous in data analysis, including analysis of time series. But it is inherently subjective: different users may prefer different clusterings for a particular dataset. Semi-supervised clustering addresses this by allowing the user to provide examples of instances that should (not) be in the same cluster.&lt;/p>
&lt;p>The COBRAS-TS algorithm is a semi-supervised clustering method that can use indirect feedback from users. It clusters with the user in the loop. The user can provide indirect feedback where it is only required to tell the system for a few time series whether two time series represent the same behavior or not. As a result the COBRAS-TS system can identify clusters that are characterized by small local patterns.&lt;/p>
&lt;p>
&lt;figure >
&lt;a data-fancybox="" href="https://dtai.cs.kuleuven.be/stories/stories/post/wannes/time-series-clustering/img1_hu4ae0b56b08877aad3dec303e3d3906a2_202965_2000x2000_fit_lanczos_2.png" >
&lt;img data-src="https://dtai.cs.kuleuven.be/stories/stories/post/wannes/time-series-clustering/img1_hu4ae0b56b08877aad3dec303e3d3906a2_202965_2000x2000_fit_lanczos_2.png" class="lazyload" alt="Disconnected modes in a cluster" width="400px" height="373">
&lt;/a>
&lt;/figure>
&lt;figure >
&lt;a data-fancybox="" href="https://dtai.cs.kuleuven.be/stories/stories/post/wannes/time-series-clustering/img2_hu3fa8e371462bfbb27ad46e311cd1fe6e_45246_2000x2000_fit_lanczos_2.png" >
&lt;img data-src="https://dtai.cs.kuleuven.be/stories/stories/post/wannes/time-series-clustering/img2_hu3fa8e371462bfbb27ad46e311cd1fe6e_45246_2000x2000_fit_lanczos_2.png" class="lazyload" alt="Comparison" width="400px" height="155">
&lt;/a>
&lt;/figure>
&lt;/p>
&lt;p>An interface is provided to the user that actively asks for feedback about time series where the method is the most unsure about how to cluster them.&lt;/p>
&lt;figure >
&lt;a data-fancybox="" href="https://dtai.cs.kuleuven.be/stories/stories/post/wannes/time-series-clustering/cobras_schema_hu81961e73cefbbf74308329810cb7932d_33807_2000x2000_fit_lanczos_2.png" >
&lt;img data-src="https://dtai.cs.kuleuven.be/stories/stories/post/wannes/time-series-clustering/cobras_schema_hu81961e73cefbbf74308329810cb7932d_33807_2000x2000_fit_lanczos_2.png" class="lazyload" alt="GUI" width="400px" height="218">
&lt;/a>
&lt;/figure>
&lt;figure >
&lt;a data-fancybox="" href="https://dtai.cs.kuleuven.be/stories/stories/post/wannes/time-series-clustering/cobras_example_hu4e6b9924bf96602c22d2e8c89b3568aa_221661_2000x2000_fit_lanczos_2.png" >
&lt;img data-src="https://dtai.cs.kuleuven.be/stories/stories/post/wannes/time-series-clustering/cobras_example_hu4e6b9924bf96602c22d2e8c89b3568aa_221661_2000x2000_fit_lanczos_2.png" class="lazyload" alt="GUI" width="100%" height="549">
&lt;/a>
&lt;/figure>
&lt;p>The toolbox is available at &lt;a href="https://dtai.cs.kuleuven.be/software/cobras/">https://dtai.cs.kuleuven.be/software/cobras/&lt;/a> .&lt;/p></description></item><item><title>ML Student Projects: Dots-and-Boxes</title><link>https://dtai.cs.kuleuven.be/stories/post/wannes/ml-student-projects-dots-and-boxes/</link><pubDate>Sat, 30 Nov 2019 11:18:29 +0200</pubDate><guid>https://dtai.cs.kuleuven.be/stories/post/wannes/ml-student-projects-dots-and-boxes/</guid><description>&lt;p>This games was explored in the course &amp;ldquo;Machine Learning: Project&amp;rdquo; (Prof. Hendrik Blockeel, Dr. Wannes Meert).
Students developed agents that are able to play the Dots-and-Boxes game.&lt;/p>
&lt;p>The game implementation is available at &lt;a href="https://github.com/wannesm/dotsandboxes">https://github.com/wannesm/dotsandboxes&lt;/a> (and can be played interactively).&lt;/p>
&lt;figure >
&lt;a data-fancybox="" href="https://dtai.cs.kuleuven.be/stories/stories/post/wannes/ml-student-projects-dots-and-boxes/dotsandboxes_hub2e3427bdbb71761d7b64c762a376112_37074_2000x2000_fit_lanczos_2.png" >
&lt;img data-src="https://dtai.cs.kuleuven.be/stories/stories/post/wannes/ml-student-projects-dots-and-boxes/dotsandboxes_hub2e3427bdbb71761d7b64c762a376112_37074_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="328" height="234">
&lt;/a>
&lt;/figure>
&lt;h3 id="rules-of-the-game">Rules of the game&lt;/h3>
&lt;blockquote>
&lt;p>Starting with an empty grid of dots, two players take turns adding a single horizontal or vertical line between two unjoined adjacent dots. The player who completes the fourth side of a 1×1 box earns one point and takes another turn. (A point is typically recorded by placing a mark that identifies the player in the box, such as an initial.) The game ends when no more lines can be placed. The winner is the player with the most points. The board may be of any size. When short on time, a 2×2 board (a square of 9 dots) is good for beginners. A 5×5 is good for experts.
(
&lt;a href="https://en.wikipedia.org/wiki/Dots_and_Boxes" target="_blank" rel="noopener">Wikipedia&lt;/a>)&lt;/p>
&lt;/blockquote>
&lt;figure >
&lt;a data-fancybox="" href="https://dtai.cs.kuleuven.be/stories/stories/post/wannes/ml-student-projects-dots-and-boxes/dotsandboxes2_hu8615e540607977328b5ae3129c456413_43193_2000x2000_fit_lanczos_2.png" >
&lt;img data-src="https://dtai.cs.kuleuven.be/stories/stories/post/wannes/ml-student-projects-dots-and-boxes/dotsandboxes2_hu8615e540607977328b5ae3129c456413_43193_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="300px" height="428">
&lt;/a>
&lt;/figure>
&lt;blockquote>
&lt;p>While the game has very simple rules, it has a large number of possible states (more speciﬁcally,
$|S| = 2^{r(c+1)+(r+1)c}$ states for an $r \times c$ game), and an even larger number ($\log_2(|S|)!$) of unique games can be played. No general strategy for optimal play is known. Currently, a 4 × 5 game is the largest game that is solved.
(Barker, J. K. and Korf, R. E. Solving dots-and-boxes. AAAI 2012)&lt;/p>
&lt;/blockquote>
&lt;h3 id="agents">Agents&lt;/h3>
&lt;p>The agents available in the Dots and Boxes demo are developed by students from the Master of Computer Science at KU Leuven as part of the 2017-2018 course Machine Learning Project, taught by Prof. Hendrik Blockeel and Wannes Meert.&lt;/p>
&lt;p>The machine learning technologies used in the agents are:&lt;/p>
&lt;ul>
&lt;li>Monte-Carlo Tree Search to simulate game-play&lt;/li>
&lt;li>Q-learning to learn about good actions&lt;/li>
&lt;li>Artificial Neural Net / Deep learning to learn about good board states&lt;/li>
&lt;li>Logic rules to reason about domain specific concepts such as ‘chains’ and ‘sacrificing&amp;rsquo;&lt;/li>
&lt;/ul>
&lt;h3 id="credits">Credits&lt;/h3>
&lt;p>Hendrik Blockeel, Wannes Meert, Pieter Robberechts, Arne De Brabandere and Sebastijan Dumančić contributed to this course.&lt;/p></description></item></channel></rss>